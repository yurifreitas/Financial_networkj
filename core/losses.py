import torch
import torch.nn.functional as F

# =========================================================
# üéØ Loss Q-H√≠brida Reativa (mant√©m compatibilidade)
# =========================================================
def loss_q_hibrida(q_pred, q_tgt):
    """
    Mesmo retorno, mas com:
    - leve ru√≠do simbi√≥tico anti-satura√ß√£o
    - normaliza√ß√£o adaptativa para escapar de loss=0
    """
    # erro e fator de energia adaptativo
    err = (q_pred - q_tgt)
    energy = (err**2).mean().sqrt().detach() + 1e-6

    # perda base
    mse = F.mse_loss(q_pred, q_tgt, reduction="mean")
    hub = F.smooth_l1_loss(q_pred, q_tgt, beta=0.8, reduction="mean")

    # excita√ß√£o simbi√≥tica ‚Äî injeta ru√≠do microcontrolado no gradiente
    jitter = 1e-3 * torch.randn_like(q_pred)
    reg = 1e-6 * (q_pred ** 2).mean()

    # mistura equilibrada com fator din√¢mico
    mix = 0.7 * mse + 0.3 * hub
    loss = mix / (energy + 1e-6) + reg + (jitter * err).mean() * 0.01
    return torch.nan_to_num(loss.clamp_min(1e-9))


# =========================================================
# üìà Loss Regress√£o Reativa (mant√©m compatibilidade)
# =========================================================
def loss_regressao(y_pred, y_tgt, l2=1e-4):
    """
    Refor√ßa varia√ß√£o e evita congelamento:
    - Mant√©m retorno escalar
    - Suaviza, mas injeta energia m√≠nima
    """
    reg = F.smooth_l1_loss(y_pred, y_tgt, beta=0.5, reduction="mean")
    reg_l2 = l2 * (y_pred ** 2).mean()

    # pequena energia simbi√≥tica evita perda nula
    energ = (y_pred - y_tgt).pow(2).mean().sqrt() * 1e-3
    loss = reg + reg_l2 + energ
    return torch.nan_to_num(loss.clamp_min(1e-9))
