# =========================================================
# ğŸ¯ EtherSym Finance â€” core/collector.py (versÃ£o otimizada)
# =========================================================
# - Coleta uma etapa de experiÃªncia simbiÃ³tica
# - Move tensores para GPU de forma estÃ¡vel
# - Minimiza recompilaÃ§Ãµes do modelo entre episÃ³dios
# =========================================================

import torch
from core.utils import escolher_acao, a_to_idx

def collect_step(env, modelo, device, eps_now, replay, nbuf, total_reward_ep):
    """
    Executa uma etapa no ambiente simbiÃ³tico e registra transiÃ§Ãµes no replay buffer.
    CompatÃ­vel com Env completo (sem .state persistente).
    """
    # ğŸ“¥ Estado atual
    s_cur = getattr(env, "current_state", None)
    if s_cur is None:
        s_cur = env.reset()

    # ğŸ” Converte para tensor GPU fixo (shape constante)
    if not torch.is_tensor(s_cur):
        s_cur_t = torch.tensor(s_cur, dtype=torch.float32, device=device).unsqueeze(0)
    else:
        s_cur_t = s_cur.to(device, non_blocking=True).unsqueeze(0)

    # ğŸ¯ Escolha simbiÃ³tica da aÃ§Ã£o (no device)
    a, conf = escolher_acao(
        modelo, s_cur_t, device,
        eps_now,
        getattr(env, "capital", 0.0),
        getattr(env, "pos", 0.0)
    )

    # ğŸš€ Passo no ambiente
    sp, r, done, info = env.step(a)

    # Atualiza o total de recompensa para o episÃ³dio
    total_reward_ep += r  # Acumula a recompensa no episÃ³dio

    # ğŸ’¾ TransiÃ§Ã£o N-Step
    y_ret = float(info.get("ret", 0.0))
    nbuf.push(s_cur, a, r, y_ret)

    if len(nbuf.traj) == nbuf.n or done:
        item = nbuf.flush(sp, done)
        if item:
            s0, a0, Rn, sn, dn, y0 = item
            replay.append(s0, a_to_idx(a0), Rn, sn, float(dn), y0)

    # ğŸ” Atualiza o estado
    env.current_state = sp

    # Retorna o prÃ³ximo estado, se o episÃ³dio terminou, informaÃ§Ãµes e o total de recompensa acumulado
    return sp, done, info, total_reward_ep
