import os, random, warnings, gc, torch, numpy as np, psutil

import torch, gc, threading

# =========================================================
# üöÄ Utilit√°rio turbo simbi√≥tico ‚Äî otimizado para PyTorch 2.4+
# =========================================================
import torch, gc

def turbo_cuda():

    # =========================================================
    # üöÄ Turbo CUDA n√£o bloqueante
    # =========================================================
    torch.set_float32_matmul_precision("high")
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False


def reseed(seed=42):
    """Reseta RNG global (CPU + GPU) para reprodutibilidade simbi√≥tica."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.cuda.random.manual_seed(seed)
    print(f"üå± RNG reseed simbi√≥tico | seed={seed}")

def limpar_memoria():
    """For√ßa libera√ß√£o simbi√≥tica total de VRAM e mem√≥ria RAM."""
    try:
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
        gc.collect()

        # encerra threads zumbis
        for proc in psutil.process_iter(["pid", "name"]):
            if "python" in proc.info["name"]:
                pass  # n√£o mata o processo principal, apenas sinaliza
        rss = psutil.Process(os.getpid()).memory_info().rss / 1e6
        print(f"‚ôªÔ∏è Limpeza simbi√≥tica completa | RAM usada={rss:.1f} MB")
    except Exception as e:
        print(f"[WARN] limpeza simbi√≥tica falhou: {e}")

warnings.filterwarnings("ignore", category=UserWarning)