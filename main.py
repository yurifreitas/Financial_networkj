# =========================================================
# üåå EtherSym Finance ‚Äî main_v8f_full.py
# =========================================================
# - Double DQN + N-Step + PER + Regress√£o cont√≠nua
# - AMP + GradClip + Homeostase + Poda + Regenera√ß√£o
# - Anneal de Œµ, Temperatura e Œ≤-PER + Rollback simbi√≥tico
# =========================================================

from core.setup import setup_simbiotico
from core.scheduler import update_params
from core.collector import collect_step
from core.trainer import train_step
from core.checkpoints import check_vitoria, rollback_guard
from core.patrimonio import carregar_patrimonio_global, salvar_patrimonio_global
from core.maintenance import aplicar_poda, regenerar_sinapses, verificar_homeostase, homeostase_replay
from core.logger import log_status
from core.hyperparams import *
from memory import salvar_estado

# =========================================================
# üß† Setup simbi√≥tico inicial
# =========================================================
env, modelo, alvo, opt, replay, nbuf, scaler = setup_simbiotico()
best_global = carregar_patrimonio_global()

EPSILON, lr_now, temp_now, beta_per = 1.0, LR, TEMP_INI, BETA_PER_INI
ema_q = ema_r = 0.0
total_steps = 0
last_loss = 0.0
last_y_pred = 0.0
episodio = 0
cooldown_until = 0
rollbacks = 0
last_good = None

print(f"üß† Iniciando treino simbi√≥tico | device={DEVICE.type} | patrim√¥nio inicial={best_global:.2f}")

# =========================================================
# üîÅ Loop principal (Epis√≥dios)
# =========================================================
while True:
    episodio += 1
    s = env.reset()
    env.current_state = s  # üîπ Mant√©m estado inicial coerente para o collector
    done = False
    total_reward_ep = 0.0
    capital = CAPITAL_INICIAL
    max_patrimonio = CAPITAL_INICIAL
    melhor_patrimonio_ep = CAPITAL_INICIAL

    while not done:
        total_steps += 1

        # üîß Atualiza par√¢metros din√¢micos (Œµ, Œ≤, temperatura, LR)
        lr_now, EPSILON, temp_now, beta_per, eps_now = update_params(
            total_steps, lr_now, EPSILON, temp_now, beta_per, replay, opt
        )

        # üéØ Coleta de experi√™ncia simbi√≥tica
        # collector.py cuida de executar o step e registrar replay
        s, done, info, total_reward_ep = collect_step(
            env, modelo, DEVICE, eps_now, replay, nbuf, total_reward_ep
        )

        # üß† Treino simbi√≥tico (Q + Regress√£o)
        can_train = (len(replay) >= MIN_REPLAY) and (total_steps >= cooldown_until)
        if can_train:
            loss, ema_q, ema_r = train_step(modelo, alvo, opt, replay, scaler, ema_q, ema_r, total_steps)
            last_loss = loss if loss is not None else last_loss

            # Guard: rollback autom√°tico em caso de perda de estabilidade
            if rollback_guard(loss, total_steps, modelo, alvo, opt, replay, EPSILON):
                cooldown_until = total_steps + COOLDOWN_STEPS
                rollbacks += 1
                print(f"‚ö† Reset simbi√≥tico #{rollbacks} | cooldown at√© {cooldown_until}")

        # üèÜ Vit√≥ria simbi√≥tica (melhor patrim√¥nio)
        best_global = check_vitoria(
            info.get("patrimonio", 0.0),
            best_global,
            modelo,
            opt,
            replay,
            EPSILON,
            total_reward_ep,
        )

        # üåø Poda + regenera√ß√£o + homeostase simbi√≥tica
        if total_steps % PODA_EVERY == 0 and len(replay) >= MIN_REPLAY:
            taxa = aplicar_poda(modelo)
            regenerar_sinapses(modelo, taxa)
            verificar_homeostase(modelo, last_loss)
            print(f"üåø Poda simbi√≥tica ‚Äî taxa={taxa*100:.2f}% | step={total_steps}")

        if total_steps % HOMEOSTASE_EVERY == 0:
            homeostase_replay(replay)

        modelo.verificar_homeostase(last_loss)

        # üßæ Logs peri√≥dicos
        if total_steps % PRINT_EVERY == 0:
            energia = float(info.get("energia", 1.0))
            last_y_pred = float(info.get("ret", 0.0))
            log_status(
                episodio, total_steps,
                float(info.get("capital", capital)),
                float(info.get("patrimonio", 0.0)),
                float(info.get("max_patrimonio", max_patrimonio)),
                eps_now, temp_now, beta_per, lr_now,
                energia, last_y_pred, last_loss
            )

        # üíæ Checkpoints peri√≥dicos
        if total_steps % SAVE_EVERY == 0 and len(replay) >= MIN_REPLAY:
            salvar_estado(modelo, opt, replay, EPSILON, total_reward_ep)
            print(f"üíæ Checkpoint salvo | step={total_steps}")

        # üíæ Snapshot de rollback
        if total_steps % ROLLBACK_EVERY == 0 and len(replay) >= MIN_REPLAY:
            last_good = {
                "model": {k: v.detach().cpu().clone() for k, v in modelo.state_dict().items()},
                "target": {k: v.detach().cpu().clone() for k, v in alvo.state_dict().items()},
                "opt": opt.state_dict(),
                "eps": float(EPSILON),
                "lr": float(lr_now),
                "temp": float(temp_now),
            }

    # =========================================================
    # üîö Encerramento do epis√≥dio simbi√≥tico
    # =========================================================
    salvar_patrimonio_global(best_global)
    print(f"\nüîÅ Epis√≥dio {episodio:04d} finalizado | cap_final={info.get('capital', 0):.2f} | best={best_global:.2f}\n")

    if info.get("capital", 0) <= 1.0:
        print("üíÄ Fal√™ncia simbi√≥tica ‚Äî reiniciando ambiente")
        s = env.reset()
        env.current_state = s  # üîπ ressincroniza o estado p√≥s-reset
