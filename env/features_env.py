# =========================================================
# üåå EtherSym Finance ‚Äî features_env_v2_cached.py
# =========================================================
# - Gera features simbi√≥ticas completas (estat√≠sticas, fractais, energ√©ticas e t√©cnicas)
# - Usa cache autom√°tico para evitar rec√°lculo desnecess√°rio
# - Verifica integridade pelo hash do dataset de entrada
# - Compat√≠vel com o ambiente Env do treino simbi√≥tico
# =========================================================

import numpy as np
import pandas as pd
import hashlib, os, time, json

# === Indicadores internos ===
from indicators.statistical.hurst import compute as hurst
from indicators.statistical.shannon_entropy import compute as shannon_entropy
from indicators.statistical.kurtosis import compute as kurtosis
from indicators.signal_energy.wavelet_transform import compute as wavelet_energy
from indicators.technical.macd import compute as macd
from indicators.technical.rsi import compute as rsi
from indicators.technical.trix import compute as trix
from indicators.fractal_chaos.mfdfa import compute as multifractal_dfa


# =========================================================
# üß© Fun√ß√µes auxiliares
# =========================================================
def normalize(series: pd.Series) -> pd.Series:
    return (series - series.mean()) / (series.std() + 1e-9)


def hash_dataframe(df: pd.DataFrame) -> str:
    """Gera hash simbi√≥tico para o dataset (para verificar se j√° existe cache)."""
    h = hashlib.sha256()
    h.update(str(df.shape).encode())
    h.update(str(df.head(100).to_dict()).encode())
    return h.hexdigest()[:16]


# =========================================================
# üå† N√∫cleo simbi√≥tico com cache
# =========================================================
def make_feats(df: pd.DataFrame, cache_dir="cache_features", force=False):
    """
    Retorna (base, price) e salva automaticamente um cache.
    - cache_dir: diret√≥rio onde os .npz ser√£o salvos
    - force: se True, recalcula mesmo se j√° existir cache v√°lido
    """

    os.makedirs(cache_dir, exist_ok=True)
    df = df.copy()
    df.columns = [c.lower() for c in df.columns]
    data_hash = hash_dataframe(df)
    cache_file = os.path.join(cache_dir, f"features_{data_hash}.npz")
    meta_file = cache_file.replace(".npz", ".json")

    # üîç Se j√° existe cache v√°lido
    if not force and os.path.exists(cache_file):
        data = np.load(cache_file)
        base, price = data["base"], data["price"]
        print(f"‚ö° Cache simbi√≥tico carregado: {cache_file} | base={base.shape}")
        return base, price

    print("üîÑ Calculando features simbi√≥ticas (pode demorar alguns minutos)...")
    start = time.time()

    # ===============================
    # üîπ Retornos e volatilidade b√°sica
    # ===============================
    df["ret"] = df["close"].pct_change().fillna(0.0)
    df["vol_ret"] = df["ret"].rolling(24).std().fillna(0.0)

    # ===============================
    # üîπ Indicadores T√©cnicos
    # ===============================
    _, _, macd_hist = macd(df["close"].values)
    df["macd_hist"] = macd_hist
    df["rsi_n"] = (rsi(df["close"].values, period=14) - 50) / 50
    df["trix"] = trix(df["close"].values, period=15)

    # M√©dias m√≥veis diferenciais
    df["ema_fast"] = df["close"].ewm(span=12).mean()
    df["ema_slow"] = df["close"].ewm(span=26).mean()
    df["ema_diff"] = (df["ema_fast"] - df["ema_slow"]) / (df["close"] + 1e-9)

    # Corpo e amplitude das velas
    df["range"] = (df["high"] - df["low"]) / (df["close"] + 1e-9)
    df["body"] = (df["close"] - df["open"]) / ((df["high"] - df["low"]) + 1e-9)

    # ===============================
    # üîπ Volume e desvio Z
    # ===============================
    df["volume_z"] = normalize(
        df["volume"].rolling(48).apply(lambda x: x.iloc[-1] - x.mean())
    )
    roll = df["close"].rolling(48)
    df["z"] = ((df["close"] - roll.mean()) / (roll.std() + 1e-9)).fillna(0.0)

    # ===============================
    # üî∏ M√©tricas Estat√≠sticas & Fractais
    # ===============================
    df["hurst"] = df["close"].rolling(200).apply(lambda x: hurst(x.values).mean(), raw=False)
    df["entropy"] = df["close"].rolling(200).apply(lambda x: shannon_entropy(x.values).mean(), raw=False)
    df["kurtosis"] = df["close"].rolling(200).apply(lambda x: kurtosis(x.values).mean(), raw=False)

    # Multifractalidade (dimens√£o fractal local)
    df["mfdfa"] = df["close"].rolling(300).apply(
        lambda x: multifractal_dfa(x.values).get("dfa_alpha", np.nan), raw=False
    )

    # ===============================
    # üîπ Energia de Sinal (Wavelet)
    # ===============================
    df["wave_energy"] = df["close"].rolling(256).apply(
        lambda x: wavelet_energy(x.values), raw=False
    )

    # ===============================
    # üîπ Normaliza√ß√£o e limpeza
    # ===============================
    df = df.replace([np.inf, -np.inf], np.nan).dropna().reset_index(drop=True)

    # ===============================
    # üîπ Montagem final do vetor simbi√≥tico
    # ===============================
    base_cols = [
        "ret", "vol_ret", "range", "body",
        "ema_diff", "rsi_n", "macd_hist", "trix",
        "hurst", "entropy", "kurtosis", "mfdfa",
        "wave_energy", "z", "volume_z"
    ]

    base = df[base_cols].astype(np.float32).values
    price = df["close"].astype(np.float32).values

    # üíæ Salvar cache comprimido e metadados
    np.savez_compressed(cache_file, base=base, price=price)
    with open(meta_file, "w") as f:
        json.dump({
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "hash": data_hash,
            "n_samples": len(df),
            "n_features": base.shape[1],
            "duration_sec": round(time.time() - start, 2)
        }, f, indent=2)

    print(f"‚úÖ Features simbi√≥ticas salvas em {cache_file} ({base.shape[0]}x{base.shape[1]})")
    return base, price
