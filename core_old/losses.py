# =========================================================
# üßÆ EtherSym Finance ‚Äî core/losses_v8j.py (Resiliente)
# =========================================================
# - Anti-satura√ß√£o simbi√≥tica e energia m√≠nima garantida
# - Normaliza√ß√£o logar√≠tmica e suaviza√ß√£o din√¢mica
# - Mant√©m compatibilidade total com AMP e GradScaler
# =========================================================

import torch
import torch.nn.functional as F

# =========================================================
# üéØ Loss Q-H√≠brida Resiliente
# =========================================================
def loss_q_hibrida(q_pred: torch.Tensor, q_tgt: torch.Tensor) -> torch.Tensor:
    """
    Combina MSE + SmoothL1 + normaliza√ß√£o logar√≠tmica.
    Evita satura√ß√£o quando q_tgt √© muito maior que q_pred.
    """
    # Erro simbi√≥tico
    err = q_pred - q_tgt
    abs_err = err.abs() + 1e-6

    # Normaliza√ß√£o logar√≠tmica ‚Üí suaviza gradientes grandes
    norm = torch.log1p(abs_err)
    mse = F.mse_loss(q_pred, q_tgt, reduction="mean")
    huber = F.smooth_l1_loss(q_pred, q_tgt, beta=0.5, reduction="mean")

    # Mistura adaptativa
    base = 0.6 * mse + 0.4 * huber
    reg = 1e-6 * (q_pred ** 2).mean()
    jitter = 1e-4 * torch.randn_like(q_pred).mean()

    # Normaliza√ß√£o simbi√≥tica e energia m√≠nima
    energy = norm.mean().detach().clamp_min(1e-3)
    loss = (base / energy) + reg + jitter

    # Satura√ß√£o simbi√≥tica (evita perda explosiva)
    loss = torch.tanh(loss / 5.0) * 5.0
    return torch.nan_to_num(loss.clamp_min(1e-9))


# =========================================================
# üìà Loss Regress√£o Simbi√≥tica Resiliente
# =========================================================
def loss_regressao(y_pred: torch.Tensor, y_tgt: torch.Tensor, l2: float = 1e-4) -> torch.Tensor:
    """
    Suaviza e equaliza gradiente para previs√µes cont√≠nuas.
    Evita congelamento e reduz a amplitude de erro.
    """
    diff = y_pred - y_tgt
    abs_diff = diff.abs() + 1e-6

    # Log-scale normaliza√ß√£o para estabilidade
    norm = torch.log1p(abs_diff)
    smooth = F.smooth_l1_loss(y_pred, y_tgt, beta=0.5, reduction="mean")
    reg_l2 = l2 * (y_pred ** 2).mean()

    # Energia m√≠nima simbi√≥tica
    energ = norm.mean().detach().clamp_min(1e-3)
    loss = (smooth / energ) + reg_l2

    # Satura√ß√£o simbi√≥tica
    loss = torch.tanh(loss / 3.0) * 3.0
    return torch.nan_to_num(loss.clamp_min(1e-9))
